# Chat com Gemma - Ollama

AplicaÃ§Ã£o de chat completa usando Flask + Ollama + Gemma rodando em Docker.

## ğŸš€ Funcionalidades

- Chat em tempo real com modelo Gemma
- Interface web moderna e responsiva
- Ollama rodando localmente no container
- Status indicator do modelo
- Deploy automatizado com Docker

## ğŸ› ï¸ Quick Start

```bash
# Clone o repositÃ³rio
git clone https://github.com/lucasrcosta20/Teste-Hostinger.git
cd Teste-Hostinger

# Build e run com Docker Compose
docker compose up -d --build
```

## ğŸŒ Acesso

- **Local**: http://localhost:8080
- **VPS**: http://72.61.219.206:8080
- **Hostname**: http://srv1087217.hstgr.cloud:8080

## ğŸ“ Estrutura

- `app.py` - AplicaÃ§Ã£o Flask com endpoints do chat
- `templates/chat.html` - Interface web do chat
- `Dockerfile` - Container com Ollama + Flask
- `docker-compose.yml` - OrquestraÃ§Ã£o dos serviÃ§os
- `start.sh` - Script de inicializaÃ§Ã£o

## âš™ï¸ Como Funciona

1. Container inicia Ollama em background
2. Download automÃ¡tico do modelo Gemma 2B
3. Flask serve a interface web
4. Chat se conecta via API REST ao Ollama

## ğŸ”§ Deploy na VPS

```bash
# Na VPS
git clone https://github.com/lucasrcosta20/Teste-Hostinger.git
cd Teste-Hostinger
docker compose up -d --build
```

**Nota**: O primeiro build pode demorar alguns minutos para baixar o Ollama e o modelo Gemma.

## ğŸ“Š Monitoramento

- Status do Ollama: `/status`
- Modelos disponÃ­veis listados automaticamente
- Indicador visual de conexÃ£o na interface

## ğŸ¯ Tecnologias

- **Backend**: Flask + Python
- **IA**: Ollama + Gemma 2B
- **Frontend**: HTML5 + CSS3 + JavaScript
- **Deploy**: Docker + Docker Compose